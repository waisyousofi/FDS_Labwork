# -*- coding: utf-8 -*-
"""Statistical_Modelling_Multivariate_Linear_Regression_Part2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1936Kyi41d6d1DXc9BBbeEwvbozQByHXw

#code#4

Multiple regression is like linear regression, but with more than one independent value, meaning that we try to predict a value based on two or more variables.
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy as sp
import seaborn as sns
import statsmodels.api as sm
import statsmodels.tsa.api as smt
import warnings
from google.colab import drive
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

from statsmodels.stats.outliers_influence import variance_inflation_factor

#saving dataset as csv file
path='/content/drive/MyDrive/assignments/'
#reading csv file
dfrm=pd.read_csv(path+'data.csv')

dfrm.head()
#import the data

"""# checking the correlation of feature with each other"""

sns.pairplot(dfrm.iloc[:,:])

corr = dfrm.corr()
fig, ax = plt.subplots(figsize = (12,10))
sns.heatmap(corr, cmap='RdBu', annot=True, fmt = ".2f")
plt.xticks(range(len(corr.columns)),corr.columns)
plt.yticks(range(len(corr.columns)),corr.columns)
plt.show()

"""# Separating the data"""

target="Target"
#x is all indept vars
x=dfrm.loc[:, dfrm.columns != target]
y=dfrm.loc[:,target]

x=dfrm.loc[:,dfrm.columns!=target]
y=dfrm.loc[:,target]

xtrain,xtest,ytrian,ytest=train_test_split(x,y,test_size=0.30, random_state=42)

xtrain2=sm.add_constant(xtrain)
model = sm.OLS(ytrian,xtrain2)
resul=model.fit()
resul.params
print(resul.summary())

"""# What we should understand from the above summay is:

if the model is realy well then we will have a R-squared value which is closer to one.
if we add more features for better prediction, then R-squared value will get closer to one.

if adj R-squared is less then R-squared then that feature might not be relevent to model.


t test and P>|X|

In order to undertad whether a feature is relevent to target variable or not t test can help us.it checks every feature with target variable independent of other features.

H0:  feature coef value=0
H1:  feature coef value =\=0

Higher t value, indicates we may reject the null hypo.
lower t value indiactes the feature is having coef of less than 0.
lower the P>|t| we reject null hypo.
"""

xtest=sm.add_constant(xtest)
ypredict=resul.predict(xtest)
pdata={
    "Target":ypredict[0:]
}
predictdf=pd.DataFrame(pdata)
predictdf.head()

residualerror=ytest - ypredict
#residualerror=y-yhat

"""# Verifying the Assumptions of Linear Regression

1.Low or NO Multicolinearity
"""

#Lower or No multicolinearity assumtion test using
#vif(variance inflation factor)

#using vif we can verify this assumption
#if vif>10, it means heavy multicoy between that feature and other features
#vif<=5, indicates very low relation of that feature wiht others

vif=[variance_inflation_factor(xtrain.values,i) for i in range(xtrain.shape[1])]
vifdata={
    "VIF value": vif[0:]
}
pd.DataFrame(vifdata,index=xtrain.columns).T

"""from the VIF values we can understadnd that the independent variables have very low colinearity with each other, it means this assumption for the linear regression is true.

2.Normality of residual
"""

sns.distplot(residualerror)

np.mean(residualerror)

"""As we can see from the graph and the mean value, overall it is normally distribued which holds the assumption of residual error normally distributed as true."""



"""3.Homescedasticity"""

#it is all about constant variance
#predicitons on x-axis
#residuals on the y-axis

fig, ax=plt.subplots(figsize=(6,2.5))
_=ax.scatter(ypredict,residualerror)

"""the variance is almost constant at the extend that holds the assumption true"""



"""4.No autocorrelation of residuals"""

#it refers to the degree of correlation between individual variable
#with others
acfunc=smt.graphics.plot_acf(residualerror, lags=40,alpha=0.05)

"""as the graph of acf shows,
non of the lag or datapoint is crossing from the blue border in order to be autucorrelated.

"""



"""#code#5 """

import pandas as pd

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy as sp
import seaborn as sns
import statsmodels.api as sm
import statsmodels.tsa.api as smt
import warnings
from google.colab import drive
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

from statsmodels.stats.outliers_influence import variance_inflation_factor
from word2number import w2n


#saving dataset as csv file
path='/content/drive/MyDrive/assignments/'
#reading csv file
dfrm2=pd.read_csv(path+'hiring.csv')

dfrm2.head()



#dealing with null values
dfrm2.experience=dfrm2.experience.fillna("zero")
dfrm2["test_score(out of 10)"]=dfrm2["test_score(out of 10)"].fillna(dfrm2["test_score(out of 10)"].median())
dfrm2



"""# checking the correlation of feature with each other"""

sns.pairplot(dfrm2.iloc[:,:])

corr = dfrm2.corr()
fig, ax = plt.subplots(figsize = (12,10))
sns.heatmap(corr, cmap='RdBu', annot=True, fmt = ".2f")
plt.xticks(range(len(corr.columns)),corr.columns)
plt.yticks(range(len(corr.columns)),corr.columns)
plt.show()

"""# Separating the data"""

target2="salary($)"
#x is all indept vars
x2=dfrm2.loc[:,dfrm2.columns != target]
y2=dfrm2.loc[:,target2]

x2=dfrm2.loc[:,dfrm2.columns!=target2]
y2=dfrm2.loc[:,target2]

xtrain2,xtest2,ytrian2,ytest2=train_test_split(x2,y2,test_size=0.50, random_state=20)

xtrain22=sm.add_constant(xtrain2)
model = sm.OLS(ytrian2,xtrain22)
resul=model.fit()
resul.params
print(resul.summary())

"""# What we should understand from the above summay is:

if the model is realy well then we will have a R-squared value which is closer to one.
if we add more features for better prediction, then R-squared value will get closer to one.

if adj R-squared is less then R-squared then that feature might not be relevent to model.


t test and P>|X|

In order to undertad whether a feature is relevent to target variable or not t test can help us.it checks every feature with target variable independent of other features.

H0:  feature coef value=0
H1:  feature coef value =\=0

Higher t value, indicates we may reject the null hypo.
lower t value indiactes the feature is having coef of less than 0.
lower the P>|t| we reject null hypo.
"""

xtest2=sm.add_constant(xtest2)
ypredict2=resul.predict(xtest2)
pdata={
    "Salary($)":ypredict2[0:]
}
predictdf=pd.DataFrame(pdata)
predictdf

residualerror2=ytest - ypredict
#residualerror=y-yhat
residualerror2.head()

"""# Verifying the Assumptions of Linear Regression

1.Low or NO Multicolinearity
"""

#Lower or No multicolinearity assumtion test using
#vif(variance inflation factor)

#using vif we can verify this assumption
#if vif>10, it means heavy multicoy between that feature and other features
#vif<=5, indicates very low relation of that feature wiht others

vif=[variance_inflation_factor(xtrain.values,i) for i in range(xtrain.shape[1])]

vifdata={
    "VIF value": vif[0:]
}
pd.DataFrame(vifdata,index=xtrain2.columns).T

"""2.Normality of residual"""

sns.distplot(residualerror)

np.mean(residualerror)